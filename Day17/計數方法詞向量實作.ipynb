{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 瞭解如何使用計數方法詞向量與SVD\n",
    "\n",
    "再將文字資料輸入模型進行自然語言任務之前，其中一項重要的前處理即為將字詞向量化(詞嵌入word embedding)。 而將詞向量化的方法有很多，這裡我們會著重在介紹如何使用計數方法來將字詞向量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字詞前處理\n",
    "\n",
    "在進行字詞向量化之前，我們需要針對文本資料進行前置處理，將**文本資料分割成字詞(斷詞)**，再將分割後的**字詞轉換成字詞ID清單**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello', 'world'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_dic = set()\n",
    "word_dic |= set([\"hello\", \"world\"])\n",
    "word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed corpus: [[1 0 2 6 5 0 4 3]] \n",
      " word2idx: {'say': 0, 'you': 1, 'goodbye': 2, '.': 3, 'hello': 4, 'i': 5, 'and': 6} \n",
      " idx2word: {0: 'say', 1: 'you', 2: 'goodbye', 3: '.', 4: 'hello', 5: 'i', 6: 'and'}\n"
     ]
    }
   ],
   "source": [
    "#導入會使用的library\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "#定義前處理函式\n",
    "def preprocess(corpus: List[str], only_word: bool = False):\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    '''\n",
    "    word_dic = set()\n",
    "    processed_sentence = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        #將所有字詞轉為小寫\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        #移除標點符號(可以依據使用狀況決定是否要移除標點符號)\n",
    "        if only_word:\n",
    "            pattern = r'[^\\W_]+'\n",
    "            sentence = re.findall(pattern, sentence) #\"hello, world!\" --> [\"hello\", \"world\"]\n",
    "        else:\n",
    "            punctuation_list = ['.', ',', '!', '?']\n",
    "            for pun in punctuation_list:\n",
    "                sentence = sentence.replace(pun, ' ' + pun)\n",
    "            sentence = sentence.split(' ')\n",
    "        \n",
    "        #添加字詞到字典中\n",
    "        word_dic |= set(sentence)\n",
    "        processed_sentence.append(sentence)\n",
    "    \n",
    "    \n",
    "    #建立字詞ID清單\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for word in word_dic:\n",
    "        if word not in word2idx:\n",
    "            idx = len(word2idx)\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "\n",
    "    #將文本轉為ID型式\n",
    "    id_mapping = lambda x: word2idx[x]\n",
    "    \n",
    "    corpus = np.array([list(map(id_mapping, sentence)) for sentence in processed_sentence])\n",
    "\n",
    "    return corpus, word2idx, idx2word\n",
    "\n",
    "#定義簡易文本資料(使用Ch17講義中的例子)\n",
    "corpus = ['You say goodbye and I say hello.']\n",
    "\n",
    "processed_corpus, word2idx, idx2word = preprocess(corpus)\n",
    "print(f'Processed corpus: {processed_corpus} \\n word2idx: {word2idx} \\n idx2word: {idx2word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"a\": 1, \"b\": 2, \"c\": 4, \"d\": 3}\n",
    "i = lambda x: test[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f7168059d30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(i, [\"a\", \"b\", \"c\", \"d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共現矩陣\n",
    "將轉化處理過的文本資料轉化為共現矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 2],\n",
       "       [1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1],\n",
       "       [2, 0, 1, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定義共現矩陣函式\n",
    "def create_co_matrix(corpus: np.ndarray, vocab_size: int, window_size: int=1):\n",
    "    '''\n",
    "    '''\n",
    "    # initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        sentence_size = len(sentence)\n",
    "    \n",
    "        for idx, word_id in enumerate(sentence):\n",
    "            for i in range(1, window_size + 1):\n",
    "                left_idx = idx - i\n",
    "                right_idx = idx + i\n",
    "\n",
    "                if left_idx >= 0:\n",
    "                    left_word_id = sentence[left_idx]\n",
    "                    co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "                if right_idx < sentence_size:\n",
    "                    right_word_id = sentence[right_idx]\n",
    "                    co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(processed_corpus, len(word2idx), 2)\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 2],\n",
       "       [1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1],\n",
       "       [2, 0, 1, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定義共現矩陣函式\n",
    "# method two\n",
    "\n",
    "def create_co_matrix(corpus: np.ndarray, vocab_size: int, window_size: int=1):\n",
    "    '''\n",
    "    '''\n",
    "    # initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        sentence_size = len(sentence)\n",
    "    \n",
    "        for idx, word_id in enumerate(sentence):\n",
    "            left_idx = idx - window_size if idx - window_size >= 0 else 0\n",
    "            context_ids = sentence[left_idx:idx]\n",
    "            \n",
    "            for left_i, left_id in enumerate(context_ids):\n",
    "                co_matrix[word_id, left_id] += 1\n",
    "                co_matrix[left_id, word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(processed_corpus, len(word2idx), 2)\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量間相似度\n",
    "比較轉換為向量的字詞的方法有很多種，其中當要表示字詞的相似度時，最常使用的方法為餘弦相似度 (Cosine Similarity)\n",
    "\n",
    "$$\n",
    "sim(x,y) = \\frac{xy}{||x||||y||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067726510136"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定義餘弦相似度\n",
    "def cos_similarity(x: np.ndarray, y: np.ndarray, eps: float=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    \n",
    "    return np.dot(nx,ny)\n",
    "\n",
    "# calculate the similarity between I and you\n",
    "cos_similarity(co_matrix[word2idx['i']], co_matrix[word2idx['you']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立可供查詢相似度的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入字詞，查詢與此字詞top_n相似的結果\n",
    "def top_k_similarity(query: str, word2idx: dict, idx2word: dict, word_matrix: np.ndarray, top_k: int=3):\n",
    "    # handle the situation of query word not in corpus\n",
    "    if query not in word2idx:\n",
    "        raise ValueError(f\"{query} is not found in input dictionary\")\n",
    "    \n",
    "    # handle the situation of top_k is the same as the amount of words\n",
    "    if top_k >= len(word2idx):\n",
    "        raise ValueError(f\"top_k needs to be less than the amount of words\")\n",
    "        \n",
    "    print(f\"[query] : {query}\")\n",
    "    query_id = word2idx[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    similarity_scores = np.zeros(len(word2idx))\n",
    "    for i in range(len(word2idx)):\n",
    "        similarity_scores[i] = cos_similarity(query_vec, word_matrix[i])\n",
    "\n",
    "    # remove query word\n",
    "    similarity_scores[query_id] = 0\n",
    "    filter_word2idx = dict([(k, v) for k, v in word2idx.items() if k != query])\n",
    "    filter_idx2word = dict([(k, v) for k, v in idx2word.items() if k != query_id])\n",
    "    \n",
    "    # sorting by similarity score\n",
    "    top_k_idx = (-similarity_scores).argsort()[:top_k]\n",
    "    top_k_word = [filter_idx2word[word_idx] for word_idx in top_k_idx]\n",
    "    \n",
    "    return dict(zip(top_k_word, similarity_scores[top_k_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query] : you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0.8660253941251803, 'i': 0.7071067726510136, '.': 0.49999999292893216}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_similarity('you', word2idx, idx2word, co_matrix, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向點間互資訊(PPMI)\n",
    "由於共生矩陣在高頻字上的缺陷，而PMI中加入了兩字詞共同出現的機率與各自出現的機率的關係，以此解決高頻詞在共生矩陣上的缺陷。\n",
    "而PPMI即將PMI內會產生負值的情況排除(若出現負值則賦予0)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&PMI(x,y) = log_2\\frac{P(x,y)}{P(x)P(y)} = log_2\\frac{C(x,y)N}{C(x)C(y)} \\\\\n",
    "&PPMI(x,y) = max(0,PMI(x,y))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義正向點間互資訊\n",
    "\n",
    "def ppmi(co_matrix: np.ndarray, eps: float=1e-8, verbose: bool=False):\n",
    "    M = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "    N = np.sum(co_matrix) #所有字出現次數\n",
    "    S = np.sum(co_matrix, axis=0) #單一字出現次數\n",
    "    total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            pmi = np.log2(co_matrix[i, j]*N / (S[i]*S[j] + eps))\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % 10 == 0 or cnt == total:\n",
    "                    print(f\"{cnt}/{total} Done\")\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/49 Done\n",
      "20/49 Done\n",
      "30/49 Done\n",
      "40/49 Done\n",
      "49/49 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c95hyc/work_project/venv_list/nlp_marathon_py36_venv/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.8930848, 0.       , 0.8930848, 0.3081223, 0.       ,\n",
       "        0.8930848],\n",
       "       [0.8930848, 0.       , 1.7004397, 0.       , 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 1.7004397, 0.       , 0.       , 0.       , 0.7004397,\n",
       "        0.7004397],\n",
       "       [0.8930848, 0.       , 0.       , 0.       , 2.1154773, 0.       ,\n",
       "        0.       ],\n",
       "       [0.3081223, 0.       , 0.       , 2.1154773, 0.       , 1.1154772,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.7004397, 0.       , 1.1154772, 0.       ,\n",
       "        0.7004397],\n",
       "       [0.8930848, 0.       , 0.7004397, 0.       , 0.       , 0.7004397,\n",
       "        0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ppmi = ppmi(co_matrix, verbose=True)\n",
    "output_ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "觀察上面的PPMI輸出矩陣，可以發現大部分的元素都為0(稀疏矩陣)，因此可以發現此矩陣包含了許多無法提供訊息的元素，利用奇異值分解，將矩陣降維，並保存重要的資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello in co-occurrence matrix: [1 0 0 1 0 1 0]\n",
      "hello in PPMI: [0.3081223 0.        0.        2.1154773 0.        1.1154772 0.       ]\n",
      "hello in SVD: [-0.5126197  -0.5698161  -0.39725903 -0.4323913  -0.01054526 -0.124419\n",
      " -0.22839099]\n"
     ]
    }
   ],
   "source": [
    "# 使用np的linalg.svd對PPMI矩陣進行奇異值分解\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(output_ppmi)\n",
    "\n",
    "# 使用SVD將將原本的稀疏向量轉變為稠密向量\n",
    "print(f\"hello in co-occurrence matrix: {co_matrix[word2idx['hello']]}\")\n",
    "print(f\"hello in PPMI: {output_ppmi[word2idx['hello']]}\")\n",
    "print(f\"hello in SVD: {U[word2idx['hello']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0392343e-08  8.9308482e-01  4.0985121e-08  8.9308482e-01\n",
      "   3.0812228e-01  5.5780403e-09  8.9308482e-01]\n",
      " [ 8.9308476e-01 -4.0341142e-08  1.7004397e+00 -4.8094705e-08\n",
      "  -4.3083389e-08  3.7209105e-08  1.4502533e-09]\n",
      " [ 5.6385137e-09  1.7004398e+00  5.8041792e-08 -6.8160695e-09\n",
      "  -7.4432329e-08  7.0043969e-01  7.0043969e-01]\n",
      " [ 8.9308482e-01 -4.0760728e-08 -7.4080468e-08 -3.3683197e-08\n",
      "   2.1154773e+00  2.2216554e-08 -8.4676607e-08]\n",
      " [ 3.0812228e-01 -6.0040975e-08 -1.3404321e-08  2.1154773e+00\n",
      "  -5.6422177e-08  1.1154772e+00 -4.2378574e-08]\n",
      " [ 2.2115112e-08  3.5444103e-08  7.0043969e-01  2.0622537e-08\n",
      "   1.1154772e+00  2.3153314e-08  7.0043969e-01]\n",
      " [ 8.9308482e-01 -4.4883759e-08  7.0043969e-01 -8.3527041e-08\n",
      "  -4.3795090e-08  7.0043969e-01 -2.3019233e-08]]\n",
      "[[0.        0.8930848 0.        0.8930848 0.3081223 0.        0.8930848]\n",
      " [0.8930848 0.        1.7004397 0.        0.        0.        0.       ]\n",
      " [0.        1.7004397 0.        0.        0.        0.7004397 0.7004397]\n",
      " [0.8930848 0.        0.        0.        2.1154773 0.        0.       ]\n",
      " [0.3081223 0.        0.        2.1154773 0.        1.1154772 0.       ]\n",
      " [0.        0.        0.7004397 0.        1.1154772 0.        0.7004397]\n",
      " [0.8930848 0.        0.7004397 0.        0.        0.7004397 0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# 檢查分解是否正確\n",
    "A = U @ np.diag(S) @ V\n",
    "print(A)\n",
    "print(output_ppmi)\n",
    "# 可以發現做完SVD得結果跟原來的output_ppmi是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9371588  2.5547988  2.1101685  1.9556583  1.1257027  0.58972406\n",
      " 0.30812874]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.3710324 , -0.26495245,  0.31999645,  0.0807369 ,  0.45295563,\n",
       "         0.4691856 ],\n",
       "       [-0.29432744,  0.29746115, -0.5294562 ,  0.511355  ,  0.22169203,\n",
       "         0.35262936],\n",
       "       [-0.31352073, -0.30776063,  0.48896635,  0.5457005 , -0.38465765,\n",
       "        -0.12412582],\n",
       "       [-0.48203   ,  0.56445074,  0.26282662, -0.430857  , -0.33953863,\n",
       "         0.2642201 ],\n",
       "       [-0.5126197 , -0.5698161 , -0.39725903, -0.4323913 , -0.01054526,\n",
       "        -0.124419  ],\n",
       "       [-0.33312967,  0.30777904,  0.16466641,  0.03673923,  0.5294517 ,\n",
       "        -0.6964652 ],\n",
       "       [-0.26702777,  0.09261478, -0.3523957 ,  0.24547683, -0.44945022,\n",
       "        -0.26410997]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以發現前六個奇異值就佔了絕大多數的奇異值\n",
    "print(S)\n",
    "\n",
    "# 可以取前六個維度當作降為的詞向量\n",
    "U_reduce = U[:, 0:6]\n",
    "U_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 將詞向量降為二維方便視覺化\n",
    "U_visualization = U[:, 0:2]\n",
    "\n",
    "# visualization\n",
    "for word, word_id in word2idx.items():\n",
    "    plt.annotate(word, (U_reduce[word_id, 0], U_reduce[word_id, 1]))\n",
    "    \n",
    "plt.scatter(U_reduce[:, 0], U_reduce[:, 1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
